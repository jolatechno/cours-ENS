{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## implementation stollen from https://blog.paperspace.com/beginners-guide-to-boltzmann-machines-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid , save_image\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truth_func(x, y):\n",
    "    #return np.logical_xor(x, y)\n",
    "    return x*y\n",
    "    #return np.logical_or(x, y)\n",
    "    #return np.logical_not(x)\n",
    "\n",
    "def gen_xor_training(N):\n",
    "    data   = np.random.randint(0, 2, (N, 2))\n",
    "    target = truth_func(data[:, 0:1], data[:, 1:2])\n",
    "    return data,target\n",
    "\n",
    "def xor_to_flat(data, target):\n",
    "    concat = np.concatenate((data, target), axis=1).astype(np.single) #.flatten()\n",
    "    #concat = 2*concat - 1\n",
    "    return concat\n",
    "\n",
    "def flat_to_xor(data):\n",
    "    #data = data.reshape(-1, 3)\n",
    "    #data = (data + 1)/2\n",
    "    return data[:, 0:2], data[:, 2:3]\n",
    "\n",
    "def xor_sucess_rate(data, target):\n",
    "    ground_truth = truth_func(data[:, 0:1], data[:, 1:2])\n",
    "    return 1 - np.mean(np.abs(target - ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a descrete RBM\n",
    "class RBM(nn.Module):\n",
    "   def __init__(self,\n",
    "               n_vis,\n",
    "               n_hin,\n",
    "               k):\n",
    "        super(RBM, self).__init__()\n",
    "        self.W = nn.Parameter(torch.randn(n_hin,n_vis)*1e-2)\n",
    "        self.v_bias = nn.Parameter(torch.zeros(n_vis))\n",
    "        self.h_bias = nn.Parameter(torch.zeros(n_hin))\n",
    "        self.k = k\n",
    "    \n",
    "   def sample_from_p(self,p):\n",
    "       return torch.relu(torch.sign(p - Variable(torch.rand(p.size()))))\n",
    "\n",
    "   def v_to_h(self,v):\n",
    "        p_h = F.sigmoid(F.linear(v,self.W,self.h_bias))\n",
    "        sample_h = self.sample_from_p(p_h)\n",
    "        return p_h,sample_h\n",
    "    \n",
    "   def h_to_v(self,h):\n",
    "        p_v = F.sigmoid(F.linear(h,self.W.t(),self.v_bias))\n",
    "        sample_v = self.sample_from_p(p_v)\n",
    "        return p_v,sample_v\n",
    "\n",
    "   def forward(self,v):\n",
    "        pre_h1,h1 = self.v_to_h(v)\n",
    "        \n",
    "        h_ = h1\n",
    "        for _ in range(self.k):\n",
    "            pre_v_,v_ = self.h_to_v(h_)\n",
    "            pre_h_,h_ = self.v_to_h(v_)\n",
    "        \n",
    "        return v,v_\n",
    "\n",
    "   def free_energy(self,v):\n",
    "        vbias_term = v.mv(self.v_bias)\n",
    "        wx_b = F.linear(v, self.W, self.h_bias)\n",
    "        hidden_term = wx_b.exp().add(1).log().sum(1)\n",
    "        return (-hidden_term - vbias_term).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss and sucess rate for 0 epoch: -0.009890688599752406, 63%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[304], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m loss_\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     20\u001b[0m train_op\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 21\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m train_op\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     24\u001b[0m sucess_rate \u001b[38;5;241m=\u001b[39m xor_sucess_rate(\u001b[38;5;241m*\u001b[39mflat_to_xor(v1\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()))\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 200\n",
    "rbm = RBM(3, 5, k=5)\n",
    "train_op = optim.SGD(rbm.parameters(), 0.1)\n",
    "\n",
    "losses = []\n",
    "sucess_rates = []\n",
    "\n",
    "for epoch in range(20):\n",
    "    loss_ = []\n",
    "    sucess_rate_ = []\n",
    "    for i in range(500):\n",
    "        data,target = gen_xor_training(batch_size)\n",
    "        data_var = Variable(torch.from_numpy(xor_to_flat(data, target).astype(np.double)))\n",
    "        \n",
    "        v,v1 = rbm(data_var)\n",
    "        loss = rbm.free_energy(v) - rbm.free_energy(v1)\n",
    "        #loss = torch.abs(v - v1).mean()\n",
    "        \n",
    "        loss_.append(loss.data)\n",
    "        train_op.zero_grad()\n",
    "        loss.backward()\n",
    "        train_op.step()\n",
    "\n",
    "        sucess_rate = xor_sucess_rate(*flat_to_xor(v1.detach().numpy()))\n",
    "        sucess_rate_.append(sucess_rate)\n",
    "\n",
    "    losses.append(np.mean(loss_))\n",
    "    sucess_rates.append(np.mean(sucess_rate_))\n",
    "    print(\"Training loss and sucess rate for {} epoch: {}, {}%\".format(epoch, losses[-1], int(sucess_rates[-1]*100)))\n",
    "    \n",
    "print(\"\\nv_bias:\", rbm.v_bias.detach().numpy())\n",
    "print(\"h_bias:\", rbm.h_bias.detach().numpy())\n",
    "print(\"W:\\n\", rbm.W.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(v.reshape(batch_size, 3)[:20, :])\n",
    "print(v1.detach().numpy().reshape(batch_size, 3)[:20, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous RBM\n",
    "class RBM_C(nn.Module):\n",
    "    def __init__(self,\n",
    "                n_vis,\n",
    "                n_hin,\n",
    "                k):\n",
    "        torch.set_default_dtype(torch.double)\n",
    "        super(RBM_C, self).__init__()\n",
    "        \n",
    "        self.W      = nn.Parameter(torch.randn(n_hin, n_vis)*1e-2)\n",
    "        self.v_bias = nn.Parameter(torch.zeros(n_vis))\n",
    "        self.h_bias = nn.Parameter(torch.zeros(n_hin))\n",
    "        self.k = k\n",
    "        \n",
    "        self.eps=1e-2\n",
    "\n",
    "    def v_to_h(self,v):\n",
    "        p_h  = F.linear(v, self.W, self.h_bias)\n",
    "        p_h_ = p_h + torch.sign(p_h)*self.eps\n",
    "        p_h_[p_h_ == 0] = self.eps\n",
    "\n",
    "        lim_up = 1 - torch.exp(-p_h_)\n",
    "        lim_up = torch.clamp(lim_up, min=-1 + 0.1)\n",
    "        \n",
    "        y = Variable(torch.rand(p_h_.size()))*lim_up\n",
    "        h = torch.log(1 - y).div(-p_h_)\n",
    "\n",
    "        return h\n",
    "    \n",
    "    def h_to_v(self, h):\n",
    "        p_v  = F.linear(h, self.W.t(), self.v_bias)\n",
    "        p_v_ = p_v + torch.sign(p_v)*self.eps\n",
    "        p_v_[p_v_ == 0] = self.eps\n",
    "\n",
    "        lim_up = 1 - torch.exp(-p_v_)\n",
    "        lim_up = torch.clamp(lim_up, min=-1 + 0.1)\n",
    "        \n",
    "        y = Variable(torch.rand(p_v_.size()))*lim_up\n",
    "        v = torch.log(1 - y).div(-p_v_)\n",
    "\n",
    "        return v\n",
    "\n",
    "    def forward(self,v):\n",
    "        h1 = self.v_to_h(v)\n",
    "        \n",
    "        h_ = h1\n",
    "        for i in range(self.k):\n",
    "            v_ = self.h_to_v(h_)\n",
    "            h_ = self.v_to_h(v_)\n",
    "        \n",
    "        return v,v_\n",
    "    \n",
    "    def free_energy(self,v):\n",
    "        vbias_term = v.mv(self.v_bias)\n",
    "        wx_b = F.linear(v, self.W, self.h_bias)\n",
    "        hidden_term = wx_b.exp().add(1).log().sum(1)\n",
    "        return (-hidden_term - vbias_term).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss and sucess rate for 0 epoch: -0.12758637770081757, 62%\n",
      "Training loss and sucess rate for 1 epoch: -0.17742360799709353, 63%\n",
      "Training loss and sucess rate for 2 epoch: -0.17964864919233375, 62%\n",
      "Training loss and sucess rate for 3 epoch: -0.19802568584431401, 62%\n",
      "Training loss and sucess rate for 4 epoch: -0.22849368493296013, 61%\n",
      "Training loss and sucess rate for 5 epoch: -0.2940543604560367, 60%\n",
      "Training loss and sucess rate for 6 epoch: -0.5785320498036836, 57%\n",
      "Training loss and sucess rate for 7 epoch: -1.507186619984551, 54%\n",
      "Training loss and sucess rate for 8 epoch: -3.427162593724862, 52%\n",
      "Training loss and sucess rate for 9 epoch: -5.97065926604967, 50%\n",
      "Training loss and sucess rate for 10 epoch: -8.938049953934128, 49%\n",
      "Training loss and sucess rate for 11 epoch: -11.736259608300935, 48%\n",
      "Training loss and sucess rate for 12 epoch: -14.797369136603125, 49%\n",
      "Training loss and sucess rate for 13 epoch: -17.860300767517757, 49%\n",
      "Training loss and sucess rate for 14 epoch: -20.98332159076792, 49%\n",
      "Training loss and sucess rate for 15 epoch: -23.878358551283878, 50%\n",
      "Training loss and sucess rate for 16 epoch: -26.97234867983509, 50%\n",
      "Training loss and sucess rate for 17 epoch: -30.025726177044117, 51%\n",
      "Training loss and sucess rate for 18 epoch: -32.75168312209659, 51%\n",
      "Training loss and sucess rate for 19 epoch: -36.42642987295776, 51%\n",
      "\n",
      "v_bias: [15.04312923 -0.62756968  0.13144912]\n",
      "h_bias: [1.4940416  1.49878169 1.50063956 1.49800031 1.50600925]\n",
      "W:\n",
      " [[ 1.28392721e+01 -8.57553600e-04 -3.93173739e-01]\n",
      " [ 1.28291715e+01 -9.53796396e-04 -3.97317694e-01]\n",
      " [ 1.27911382e+01 -7.44608106e-04 -3.94288736e-01]\n",
      " [ 1.28451343e+01 -6.90285764e-04 -3.95403608e-01]\n",
      " [ 1.27874721e+01 -6.84369883e-04 -3.96532866e-01]]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 200\n",
    "rbm = RBM_C(3, 5, k=5)\n",
    "train_op = optim.SGD(rbm.parameters(), 0.01)\n",
    "\n",
    "losses = []\n",
    "sucess_rates = []\n",
    "\n",
    "for epoch in range(20):\n",
    "    loss_ = []\n",
    "    sucess_rate_ = []\n",
    "    for i in range(200):\n",
    "        data,target = gen_xor_training(batch_size)\n",
    "        data_var = Variable(torch.from_numpy(xor_to_flat(data, target).astype(np.double)))\n",
    "        \n",
    "        v,v1 = rbm(data_var)\n",
    "        loss = rbm.free_energy(v) - rbm.free_energy(v1)\n",
    "        \n",
    "        loss_.append(loss.data)\n",
    "        train_op.zero_grad()\n",
    "        loss.backward()\n",
    "        train_op.step()\n",
    "\n",
    "        sucess_rate = xor_sucess_rate(*flat_to_xor(v1.detach().numpy()))\n",
    "        sucess_rate_.append(sucess_rate)\n",
    "\n",
    "    losses.append(np.mean(loss_))\n",
    "    sucess_rates.append(np.mean(sucess_rate_))\n",
    "    print(\"Training loss and sucess rate for {} epoch: {}, {}%\".format(epoch, losses[-1], int(sucess_rates[-1]*100)))\n",
    "    \n",
    "print(\"\\nv_bias:\", rbm.v_bias.detach().numpy())\n",
    "print(\"h_bias:\", rbm.h_bias.detach().numpy())\n",
    "print(\"W:\\n\", rbm.W.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.34381249e-02 9.98845079e-01 7.25670195e-01]\n",
      " [6.89076978e-02 3.20080812e-01 4.52135134e-01]\n",
      " [3.60916064e-03 6.44501470e-01 4.53499518e-01]\n",
      " [2.17591723e-02 4.46516540e-01 5.27059602e-01]\n",
      " [2.88576909e-02 9.51125585e-01 5.88168672e-01]\n",
      " [4.08472139e-02 2.75099894e-01 8.12144877e-01]\n",
      " [2.88160936e-03 9.49134635e-01 4.21803481e-01]\n",
      " [2.24940596e-02 5.94745548e-01 2.53419331e-01]\n",
      " [6.53212788e-04 8.35104956e-01 1.12892377e-01]\n",
      " [1.51665856e-02 1.47207501e-01 2.10933987e-01]\n",
      " [8.51292044e-03 8.37180824e-02 9.67250632e-01]\n",
      " [1.93526801e-03 8.10765649e-01 6.00704767e-01]\n",
      " [7.97965054e-02 3.31486453e-02 8.19776312e-01]\n",
      " [1.12509342e-01 6.93695094e-01 5.18540718e-01]\n",
      " [5.43909451e-05 9.18096211e-01 5.28081426e-01]\n",
      " [7.97623108e-03 3.34002242e-01 3.68821547e-01]\n",
      " [8.57434785e-03 1.64113698e-01 9.10566478e-01]\n",
      " [3.16243466e-03 5.94430701e-01 5.29796614e-01]\n",
      " [3.53399191e-02 3.10819375e-01 7.93509706e-01]\n",
      " [3.39267104e-02 5.91438615e-01 3.71228191e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(v1.detach().numpy().reshape(batch_size, 3)[:20, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
