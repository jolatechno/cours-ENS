{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Multi class classification\n",
    "\n",
    "\n",
    "The multi-class percpetron can be implemented this way. \n",
    "We denote $K$ the number of classes, $N$ the number of (training) examples, $D$ the dimension of the data (after feature augmentation, at least with a \"1\" as first component).\n",
    "\n",
    "The **output** of the network *(not equal to the predicted label)*, can be taken as the **softmax** among the $K$ separating hyperplanes (each hyperplane $\\vec{w}_k$ separates class $k$ from the others).\n",
    "$$ y_k^{(n)} = \\text{softmax}\\big( (\\vec{w}_{k} \\cdot \\vec{x}^{(n)})_{k=1...K} \\big) = \\frac{ \\exp(  \\vec{w}_k\\cdot\\vec{x}^{(n)}   )}{\\sum_\\ell \\exp(  \\vec{w}_\\ell\\cdot\\vec{x}^{(n)})}$$\n",
    "This output can be **interpreted as the probability** that example $x^{(n)}$ belongs to the class $k$, according the classifier's current parameters\n",
    "Indeed, one can easily check that for any $\\vec{x}$, the sum of probabilities is indeed one : $\\sum_k y_k = 1$.\n",
    "The **total output of the network** is a vector $\\vec{y}^{(n)} = \\begin{pmatrix}y_1^{(n)} \\\\ y_2^{(n)} \\\\ .. \\\\ y_K^{(n)} \\end{pmatrix}$ (for the sample number $n$).\n",
    "\n",
    "The **true labels (ground truth)** of example $\\vec{x}^{(n)}$ is then encoded as a one-hot vector, so that if the example is of the second class, it may be written: $\\vec{t}^{n} = \\begin{pmatrix} 0 \\\\ 1 \\\\ 0 \\\\ .. \\\\ 0 \\end{pmatrix}$. (where $\\vec{t}^{(n)}$ or $\\vec{t}^{n}$ is for **T**ruth and is shorter to write than $\\vec{y}^{GT,(n)}$). More generally, the components $t_{n,k}$ of vector $\\vec{t}_n$ may be written using the Kronecker's delta: $t_{n,k} = \\delta(k, k_{true}^n)$, where $k_{true}^n$ is the true class of example number $n$.\n",
    "\n",
    "From now on, **we drop the superscrip $a^{(n)}$ and instead write $a_n$ or just $a$**, when it's clear enough that the quantity $a$ relates to a single example, of generic index $n$. This helps to lighten the notations.\n",
    "\n",
    "The Loss function that we should use is called the **cross-entropy loss function**, and is:\n",
    "\n",
    "$$J = \\frac1N \\sum_n^N H(\\vec{t}_{n}, \\vec{y}_{n})$$\n",
    "\n",
    "where the cross-entropy is a non-symmetric function: $$H(\\vec{t}_{n}, \\vec{y}_{n}) = -\\sum_k^K t_{n,k} \\log (y_{n,k})$$ \n",
    "\n",
    "Make sure you undersand all of the above. Write down the Loss function for the multi-class perceptron. \n",
    "### Part 2.1\n",
    "- What are the parameters of the model ? **How many real numbers is that ?** Count them in terms of $N,K,D, etc$. \n",
    "- (3-4 points) **Derive the update steps for the gradient**. (you can get inspiration from TD4.1)\n",
    "- Some Hints:\n",
    "    - It is recommended to compute the quantity $\\nabla_{w_\\ell} y_k$ ($\\ell\\neq k$) and the quantity $\\nabla_{w_k} y_k$. Try to express these simply, by recognizing $y$ when it appears. First treat the two cases separately, then try to unite the two cases in a single mathematical form, using Kronecker's delta : $\\delta(i,j)= \\{1$ if $i=j$, else $0\\}$.\n",
    "    - When there is a sum $\\sum_\\ell f(w_\\ell)$ and you derive with respect to $w_k$, the output only depends on the term $f(w_k)$ \n",
    "    - In the sum above, $\\sum_\\ell f(w_\\ell)$ the index $\\ell$ is a \"mute\" index: you can use any letter for it. Be careful not to use a letter that already exists outside the sum ($\\ell$ is like a local variable, don't use the same name for a \"global variable\" from outside the function !)\n",
    "    - For any functions $u,v$ that admit derivatives, $\\partial_x \\frac{u(x)}{v(x)} = \\frac{u'(x)v(x)-u(x)v'(x)}{(v(x))^2}$. It extends to $\\nabla_x$ without problem.\n",
    "    - $\\nabla_x \\exp(u(x)) =  \\exp(u(x)) \\nabla_x u(x)$.\n",
    "    - $\\frac{a}{1+a} = 1- \\frac{1}{1+a}$\n",
    "    - $\\partial_x \\log(u(x)) = \\frac{u'(x)}{u(x)}$ \n",
    "    - If you are too much blocked, you can ask me (via discord, in Private Message) for the solution of $\\nabla_{w_k} y_k$ and/or the solution for $\\nabla_{w_\\ell} y_k$ ($\\ell\\neq k$).\n",
    "    - In the end, the update step for the parameters that you should find is : $$ \\vec{w}_\\ell \\mapsto \\vec{w}_\\ell + \\eta \\frac1N \\sum_n^N \\vec{x}_n (\\delta_{\\ell, k_{true}^n}- y_{\\ell,n})$$\n",
    "    - If you cannot find the equation above, you can just skip this question and use it to make your program.\n",
    "    \n",
    "    \n",
    "\n",
    "### Part 2.2\n",
    "- (3 points) **Think up of all the functions you need to write**, and **put them in a class** (you can get inspiration from the correction of TP3.2) - first write a class skeleton, and **only then, write the methods** inside\n",
    "- Hints:\n",
    "    - there may be numerical errors (NaNs) because $\\exp(..)$ is too large. You can ease this by noticing the following: for any positive constant $C$, we have $$\\frac{ \\exp( a_k  )}{\\sum_\\ell \\exp (a_\\ell) }  = \\frac{C \\exp( a_k  )}{C \\sum_\\ell \\exp (a_\\ell) }= \\frac{\\exp( a_k +\\log C )}{\\sum_\\ell \\exp (a_\\ell +\\log C) }$$\n",
    "    - with this trick, when your arguments in the softmax are too large, you can simply subtract a big constant $\\log C$ from its argument, and this will reduce the chances of numerical error, without changing the result. It's a good idea to change the $w $'s with this kind of trick.\n",
    "    - it's a good idea to define the target labels (ground truth) data in one-hot vectors (as said above), compute them once and for all, and then you never have to compute them again. In practice, you may notice that for an example with label $k_{true}$, then the genreic component number of $k$ of the vector $\\vec{t}$ reads: $t_{k} = \\delta_{k, k_{true}}$\n",
    "    - the initial $w$ should be random (not all zeros), preferably, but not too big. A good idea is to have their dispersion be of order $1/D$ at most.\n",
    "    \n",
    "For this question, the main goal is to make a theoretically-working, rather clean code, using numpy array-operations (`np.dot`) and not loops, as much as possible. If you manage to do that, you will most likely have a working code (and fast code!)\n",
    "- (1 point) Test your algorithm on Fashion-MNIST: make a train / validation / test split , fit the model, compute the cross-val error, and the test error. Don't waste time on optimizing hyper-parameters (just take an $\\eta$ small enough that you kind of converge. The goal is really to prove that your algorithm does not always crash :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## remark:\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(X):\n",
    "    unique_val = np.sort(np.unique(X))\n",
    "    \n",
    "    Y = np.zeros((len(X), len(unique_val)))\n",
    "    for i in range(len(X)):\n",
    "        idx = np.where(unique_val == X[i])\n",
    "        Y[i, idx] = 1\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_digits()\n",
    "data    = dataset['data']\n",
    "labels  = dataset['target']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data, one_hot_encode(labels), test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class network:\n",
    "    def __init__(self, n_class, n_dims):\n",
    "        self.weights = np.random.rand(n_dims, n_class)/n_dims\n",
    "\n",
    "    # inference functions:\n",
    "    def inference(self, x):\n",
    "        # utility function:\n",
    "        def softmax(y):\n",
    "            offset = np.max(y.T, axis=0)\n",
    "            exp_y  = np.exp(y.T - offset)\n",
    "            exp_y  = exp_y/np.sum(exp_y, axis=0)\n",
    "            return exp_y.T\n",
    "            \n",
    "        predictions = x @ self.weights\n",
    "        predictions = softmax(predictions)\n",
    "        return predictions\n",
    "    def hard_inference(self, x):\n",
    "        predictions     = self.inference(x)\n",
    "        hard_prediction = np.argmax(predictions, axis=1)\n",
    "        return hard_prediction\n",
    "\n",
    "    # train functions:\n",
    "    def train(self, x, t, rate=0.01):\n",
    "        predictions       = self.inference(x)\n",
    "        prediction_delta  = t - predictions\n",
    "        x_time_prediction = np.expand_dims(x, 2)*np.expand_dims(prediction_delta, 1)\n",
    "        delta_weights     = np.mean(x_time_prediction, axis=0)\n",
    "        self.weights      = self.weights + rate*delta_weights\n",
    "\n",
    "    # test functions:\n",
    "    def test_entropy(self, x, t):\n",
    "        predictions = self.inference(x)\n",
    "        h           = -np.log(predictions)*t\n",
    "        H           = np.sum(h, axis=1)\n",
    "        H_tot       = np.mean(H)\n",
    "        return H_tot\n",
    "    def test_accurcay(self, x, t):\n",
    "        hard_prediction = self.hard_inference(x)\n",
    "        if len(t.shape) == 2:\n",
    "            t = np.argmax(t, axis=1)\n",
    "        return np.mean(hard_prediction == t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12794612794612795\n",
      "0.2255892255892256\n",
      "0.2558922558922559\n",
      "0.5168350168350169\n",
      "0.5572390572390572\n",
      "0.5707070707070707\n",
      "0.5993265993265994\n",
      "0.601010101010101\n"
     ]
    }
   ],
   "source": [
    "net = network(Y_train.shape[1], X_train.shape[1])\n",
    "\n",
    "print(net.test_accurcay(X_test, Y_test))\n",
    "\n",
    "for rate in [0.5, 0.2, 0.1, 0.05, 0.05, 0.01, 0.001]:\n",
    "    net.train(X_train, Y_train, rate)\n",
    "    print(net.test_accurcay(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-23 12:58:56.492552: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-23 12:58:56.555004: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-23 12:58:56.555079: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-23 12:58:56.557119: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-23 12:58:56.568247: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-23 12:58:56.569782: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-23 12:58:58.643491: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "(trainX3d, trainYraw), (testX3d, testYraw) = fashion_mnist.load_data()\n",
    "\n",
    "trainY, testY = one_hot_encode(trainYraw), one_hot_encode(testYraw)\n",
    "trainX, testX = trainX3d.reshape(trainX3d.shape[0], 28*28), testX3d.reshape(testX3d.shape[0], 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.091\n",
      "0.2961\n",
      "0.225\n",
      "0.1891\n",
      "0.3009\n",
      "0.3413\n",
      "0.4663\n",
      "0.4548\n"
     ]
    }
   ],
   "source": [
    "net_fashion = network(trainY.shape[1], trainX.shape[1])\n",
    "\n",
    "print(net_fashion.test_accurcay(testX, testY))\n",
    "\n",
    "for rate in [0.1, 0.1, 0.1, 0.1, 0.05, 0.01, 0.005]:\n",
    "    net_fashion.train(trainX[:15000], trainY[:15000], rate)\n",
    "    print(net_fashion.test_accurcay(testX, testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f83f47acf50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(net_fashion.weights[:, 2].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
