{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part 2. Learning word2vec in pure numpy\n",
    "\n",
    "## initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    initialize all the training parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    E = np.random.normal(loc=0, scale=.01, size=(vocab_size, emb_size))\n",
    "    W = np.random.normal(loc=0, scale=.01, size=(vocab_size, emb_size))\n",
    "  \n",
    "    parameters = {}\n",
    "    parameters['WRD_EMB'] = E\n",
    "    parameters['W'] = W\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# par exemple un plongement de dimension 5\n",
    "parameters = initialize_parameters(len(tok2id), 5)\n",
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parameters['WRD_EMB'].shape, parameters['W'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#on peut les multiplier :\n",
    "(parameters['WRD_EMB'].T @ parameters['W']).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(inds, parameters):\n",
    "    \n",
    "    m = inds.shape[1]\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    # sélectionner les lignes du plongement correspondant aux indices du batch, transformer -> autant de colonnes que d'indices\n",
    "    word_vec = WRD_EMB[inds.flatten(), :].T    \n",
    "    assert(word_vec.shape == (WRD_EMB.shape[1], m))\n",
    "    \n",
    "    m = word_vec.shape[1]\n",
    "    W = parameters['W']\n",
    "    Z = np.dot(W, word_vec)\n",
    "    assert(Z.shape == (W.shape[0], m)) \n",
    "    \n",
    "    expZ = np.exp(Z)\n",
    "    softmax_out = np.divide(expZ, np.sum(expZ, axis=0, keepdims=True) + 0.001)\n",
    "    assert(softmax_out.shape == Z.shape)\n",
    "    \n",
    "    caches = {}\n",
    "    caches['inds'] = inds\n",
    "    caches['word_vec'] = word_vec\n",
    "    caches['W'] = W\n",
    "    caches['Z'] = Z\n",
    "    return softmax_out, caches\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(softmax_out, Y):\n",
    "    \"\"\"\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m=batch_size)\n",
    "    Y: ground truth: indices à prédire. shape: (1, m)\n",
    "    \"\"\"\n",
    "    m = softmax_out.shape[1]\n",
    "    \n",
    "    # todo: comprendre ceci :\n",
    "    # du softmax out, je sélectionne les lignes correspondantes aux tokens prédits\n",
    "    # ça me donne les probabilités, j'en prends le log, et j'en fais la moyenne\n",
    "    # si la prédiction était parfaite, la proba serait 1, le log est 0 --> coût zéro\n",
    "    # sinon, j'ai une proba plus petite que 1, je fais la moyenne de ces logs de probas (qui sont tous négatifs)\n",
    "    # avec le \"-\" ça devient positif, c'est donc une mesure de l'erreur de la prédiction\n",
    "    # notez que Y peut contenir plusieurs fois le même indice à prédire. Il est comptez autant de fois qu'il devrait être prédit.\n",
    "\n",
    "    cost = -(1 / m) * np.sum(np.log(softmax_out[Y.flatten(), np.arange(Y.shape[1])] + 0.001))\n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(Y, softmax_out):\n",
    "    \"\"\"\n",
    "    Y: labels of training data. shape: (1, m=batch_size)\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m=batch_size)\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # on calcul la différence entre la prédiction et la ground truth (Y)\n",
    "    # si la prédiction était parfaite, le dL_dZ devient 0\n",
    "    # on ne touche qu'aux lignes à prédire\n",
    "    # si le token n'est pas à prédire, on ne touche pas à la ligne\n",
    "    softmax_out[Y.flatten(), np.arange(m)] -= 1.0\n",
    "    dL_dZ = softmax_out\n",
    "    \n",
    "    assert(dL_dZ.shape == softmax_out.shape)\n",
    "    return dL_dZ\n",
    "\n",
    "def dense_backward(dL_dZ, caches):\n",
    "    \"\"\"\n",
    "    dL_dZ: shape: (vocab_size, m)\n",
    "    caches: dict. results from each steps of forward propagation\n",
    "    dL_dZ a des valeurs négatives là où il faut changer qqch. le plus elles sont négatives, le plus c'est grave.\n",
    "    \"\"\"\n",
    "    W = caches['W']\n",
    "    word_vec = caches['word_vec']\n",
    "    m = word_vec.shape[1]\n",
    "    \n",
    "    # on multiplie là où il faut changer qqch :\n",
    "    # dL_dZ.shape = (vocab_size, m) \n",
    "    dL_dW = (1 / m) * np.dot(dL_dZ, word_vec.T)\n",
    "    dL_dword_vec = np.dot(W.T, dL_dZ)\n",
    "    \n",
    "    assert(W.shape == dL_dW.shape)\n",
    "    assert(word_vec.shape == dL_dword_vec.shape)\n",
    "    \n",
    "    return dL_dW, dL_dword_vec\n",
    "\n",
    "def backward_propagation(Y, softmax_out, caches):\n",
    "    dL_dZ = softmax_backward(Y, softmax_out)\n",
    "    dL_dW, dL_dword_vec = dense_backward(dL_dZ, caches)\n",
    "    \n",
    "    gradients = dict()\n",
    "    gradients['dL_dZ'] = dL_dZ\n",
    "    gradients['dL_dW'] = dL_dW\n",
    "    gradients['dL_dword_vec'] = dL_dword_vec\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, caches, gradients, learning_rate):\n",
    "    \"\"\"\n",
    "    ici on met le plongement à jour\n",
    "    \"\"\"\n",
    "    vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
    "    inds = caches['inds']\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    dL_dword_vec = gradients['dL_dword_vec']\n",
    "    m = inds.shape[-1]\n",
    "    \n",
    "    # notez qu'on ne modifie que les lignes correspondants aux mots centraux\n",
    "    WRD_EMB[inds.flatten(), :] -= dL_dword_vec.T * learning_rate\n",
    "\n",
    "    parameters['W'] -= learning_rate * gradients['dL_dW']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def skipgram_model_training(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size=256, parameters=None, print_cost=False, plot_cost=True):\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "        \n",
    "    if parameters is None:\n",
    "        parameters = initialize_parameters(vocab_size, emb_size)\n",
    "    \n",
    "    # pour l'instant ces trois variables ne sont pas utilisées\n",
    "    # todo: utiliser ces variables pour garder le meilleur modèle à la fin\n",
    "    best_epoch = 0\n",
    "    min_epoch_cost = float('inf')\n",
    "    parameters['best_embedding'] = parameters['WRD_EMB']\n",
    "    \n",
    "    begin_time = datetime.now()\n",
    "    for epoch in range(epochs):\n",
    "        epoch_cost = 0\n",
    "        batch_inds = list(range(0, m, batch_size))\n",
    "        np.random.shuffle(batch_inds)\n",
    "        for i in batch_inds:\n",
    "            X_batch = X[:, i:i+batch_size]\n",
    "            Y_batch = Y[:, i:i+batch_size]\n",
    "\n",
    "            softmax_out, caches = forward_propagation(X_batch, parameters)\n",
    "            cost = cross_entropy(softmax_out, Y_batch)\n",
    "            gradients = backward_propagation(Y_batch, softmax_out, caches)\n",
    "            update_parameters(parameters, caches, gradients, learning_rate)\n",
    "            epoch_cost += cost\n",
    "            \n",
    "        costs.append(epoch_cost)\n",
    "        # todo: ajouter ici le code pour garder le best_embedding\n",
    "        if epoch_cost<min_epoch_cost:\n",
    "            parameters['best_embedding'] = parameters['WRD_EMB']\n",
    "            best_epoch = epoch\n",
    "            min_epoch_cost = epoch_cost\n",
    "        # fin todo\n",
    "        \n",
    "        if print_cost and epoch % 200 == 0:\n",
    "            print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))\n",
    "        if epoch % (epochs // 100) == 0:\n",
    "            learning_rate *= 0.98\n",
    "    end_time = datetime.now()\n",
    "    print('training time: {}'.format(end_time - begin_time))\n",
    "    print(\"TODO I've kept the embedding of epoch {best_epoch} with cost {min_epoch_cost}.\".format(best_epoch=best_epoch,min_epoch_cost=min_epoch_cost))        \n",
    "    if plot_cost:\n",
    "        plt.plot(np.arange(epochs), costs)\n",
    "        plt.xlabel('# of epochs')\n",
    "        plt.ylabel('cost')\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape, X.shape, Y_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = skipgram_model_training(X, Y, VOCAB_SIZE, 50, 0.05, 5000, batch_size=128, parameters=None, print_cost=True)\n",
    "# tester avec une batch_size petite pour voir...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### todo:\n",
    "- garder le meilleur modèle\n",
    "- visualiser aussi le meilleur modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
