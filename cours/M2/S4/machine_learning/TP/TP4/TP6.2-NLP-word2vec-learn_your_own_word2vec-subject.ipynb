{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credit: \n",
    "Kim Gerdes, Kirian Guiller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## constants, for later\n",
    "WINDOW_SIZE = 3\n",
    "VOCAB_SIZE = 5000\n",
    "MEMORY_LIMIT = 10000\n",
    "EMBED_DIM = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A dive into embeddings\n",
    "\n",
    "## Outline:\n",
    "\n",
    "### Part 1. Data extraction, cleaning, tokenization, building a train set (skip gram)\n",
    "\n",
    "### Part 2. Learning word2vec in pure numpy\n",
    "\n",
    "### Part 3. Studying the result (learned embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preliminary steps:\n",
    "- tokenization (very simple example)\n",
    "- token-words vs token-indices \n",
    "- initialization\n",
    "- visualization\n",
    "- forward, cost function, backward, training\n",
    "- first on a single sentence, then on a bigger text\n",
    "\n",
    "Main goal: understand the successive transformations which allow to get a decent vectorial representation of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import os, time\n",
    "## handy files-loading-management: glob\n",
    "from glob import glob\n",
    "# re as Regular Expression\n",
    "import re\n",
    "import json\n",
    "\n",
    "## Counter: counting occurences easily (lazy method)\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part 1. Data extraction, cleaning, tokenization, building a train set (skip gram)\n",
    "\n",
    "We perform the following steps, first on a short sentence:\n",
    "- tokenization\n",
    "- converting to indices (almost a one-hot vector)\n",
    "- generate training data (skip gram style)\n",
    "- Same operations but for an actual corpus\n",
    "\n",
    "\n",
    "## 1.1 Tokenization \n",
    "We convert a single sentence into tokens, i.e. words or equivalent, essentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## simple tokenization:\n",
    "phrase = \"Il m’a bien ennervée, ce gars-là, avec ce copier-coller !\"\n",
    "retoken = re.compile(r\"\\w+\")\n",
    "retoken.findall(phrase.lower().replace('’',\"'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## more advanced tokenization...\n",
    "phrase = \"Il m'a bien ennervée, ce gars-là, avec ce copier-coller !\"\n",
    "retoken = re.compile(r\"\\w+'?\")\n",
    "# the simpler is to first replace:\n",
    "reclitic = re.compile(r\"-(là|ci|je|tu|il|elle|t-il|t-elle|ils|elles|t-ils|t-elles|nous|vous)\")\n",
    "phrase = reclitic.sub(r' \\1',phrase)\n",
    "retoken = re.compile(r\"[\\w-]+'?\")\n",
    "retoken.findall(phrase.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(phrase):\n",
    "    retoken = re.compile(r\"\\w+'?\")\n",
    "    retoken = re.compile(r\"[\\w-]+'?\")\n",
    "    return retoken.findall(reclitic.sub(r' \\1',phrase.lower()).replace('’',\"'\").replace('',\"'\")) \n",
    "    # problèmes d'encodage de certains fichiers : dEDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize(phrase)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Converting to indices (almost a one-hot vector)\n",
    "\n",
    "(and storing the mapping of course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(tokens).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(tokens):\n",
    "    \"\"\"\n",
    "    tokens : list of tokens\n",
    "    tok2id : dictionnary : link between tokens (in letters) and token-index\n",
    "    id2tok : dictionnary:  inverse of tok2id\n",
    "    fin todo\n",
    "    \"\"\"\n",
    "    tok2id = dict()\n",
    "    id2tok = dict()\n",
    "\n",
    "    ## TODO \n",
    "\n",
    "    return tok2id, id2tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok2id, id2tok = mapping(tokens)\n",
    "tok2id, id2tok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Create training data (skip-gram style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(tokens, tok2id, window_size):\n",
    "    \"\"\"\n",
    "    we create pairs of X,Y for training:\n",
    "    X and Y have same length: \n",
    "    X is the index of the input token,\n",
    "    Y is the one-hot (several-hot actually) encoding of the context tokens (output)\n",
    "    \"\"\"\n",
    "    vocabulary = tok2id.keys()\n",
    "    X = []\n",
    "    Y = []\n",
    "    N = len(tokens)\n",
    "    if N >= 2*window_size+1:\n",
    "        ## replace unknown words with <unk>\n",
    "        for i, tok in enumerate(tokens):\n",
    "            if tok not in vocabulary:\n",
    "                tokens[i] = \"<unk>\"\n",
    "\n",
    "        ## fill in X and Y:\n",
    "        \n",
    "        \n",
    "        ## TODO (or not, depending on time...)\n",
    "            \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's check on the first example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "X, Y = generate_training_data(tokens, tok2id, window_size)\n",
    "X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "id2tok[4], id2tok[5], id2tok[0], id2tok[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2tok[0], id2tok[1], id2tok[2], id2tok[3], id2tok[5], id2tok[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y[0][:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Same operations but now for an actual corpus\n",
    "\n",
    "- First, we need to look at word frequencies, and decide a cut-off: words (types) more frequent than this cutoff will be part of the vocabulary, less frequent types will be replaced with the tyoe \"<unk>\" (unknown)\n",
    "- Then, we open again all files and encode their sentences into one-hot vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  (1)  Data Loading, tokenization, counting types frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WARNING : of course, unzip \"Plain text\" and put it in the right place..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textfiles = [y for x in os.walk('/home/flandes/data/Plain text') for y in glob(os.path.join(x[0], '*.txt'))]\n",
    "print('We have ',len(textfiles),' text files.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(phrase):\n",
    "#     retoken = re.compile(r\"\\w+'?\")\n",
    "    retoken = re.compile(r\"[\\w-]+'?\")\n",
    "    reclitic = re.compile(r\"-(là|ci|je|tu|il|elle|t-il|t-elle|ils|elles|t-ils|t-elles|nous|vous)\")\n",
    "    return retoken.findall(reclitic.sub(r' \\1',phrase.lower()).replace('’',\"'\").replace('',\"'\")) \n",
    "    # problèmes d'encodage de certains fichiers : dEDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this Counter helps us count types occurences in a few lines:\n",
    "wordcounter = Counter()\n",
    "for f in textfiles:\n",
    "    tokenized_text = tokenize(open(f, encoding='iso-8859-15').read())\n",
    "    wordcounter.update(Counter(tokenized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounter[\"equipe\"], wordcounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Nous avons',len(wordcounter),'types et', sum(wordcounter.values()),'tokens.')\n",
    "print('Voici les 10 mots les plus courants :', wordcounter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Ordering types by their frequency, definition of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# VOCAB_SIZE = 5000\n",
    "\n",
    "## NOTE !!! IT IS VERY IMPORTANT TO ORDER THEM BY FEQUENCY, HERE !!! \n",
    "common_words_list_of_pairs = wordcounter.most_common(VOCAB_SIZE)  # sorts types by their frequency of occurence\n",
    "common_words_list_of_pairs[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df = pd.DataFrame.from_dict(wordcounter, orient='index', columns=['frequency']).sort_values(by='frequency', ascending=False)\n",
    "freq_df.plot(loglog=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Converting to indices (almost a one-hot vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a dictionnary from token to id (arbitrary ID)\n",
    "## and one from ID to token (useful for inference on other texts too !)\n",
    "tok2id = {}\n",
    "id2tok = {}\n",
    "other_words_token = \"<unk>\"  # special token to describe all other words (not in the vocab)\n",
    "\n",
    "i=0\n",
    "token = other_words_token\n",
    "tok2id[token] = i\n",
    "id2tok[i] = token\n",
    "i+=1\n",
    "for element in common_words_list_of_pairs:  ## it's crucial that we ordered types by frequency !!\n",
    "    token, freq = element\n",
    "    tok2id[token] = i\n",
    "    id2tok[i] = token\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words_list_of_pairs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounter[common_words_list_of_pairs[-1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# tok2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# id2tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we save our dicts for later use\n",
    "myjson = json.dumps(tok2id)\n",
    "f = open(\"tok2id.json\",\"w\")\n",
    "f.write(myjson)\n",
    "f.close()\n",
    "myjson = json.dumps(id2tok)\n",
    "f = open(\"id2tok.json\",\"w\")\n",
    "f.write(myjson)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vocabulary = tok2id.keys()\n",
    "# vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [optionnal] Quick study of the relationship between type length and frequency of that type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('longueur moyenne dans le dictionnaire (sur les types) :',sum([len(t) for t in wordcounter])/len(wordcounter))\n",
    "print('longueur moyenne dans le texte (sur les tokens) :',sum([len(t)*f for t,f in wordcounter.items()])/ sum(wordcounter.values()))\n",
    "print('les mots les plus longs :', sorted(wordcounter, key=lambda x:len(x), reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenfreq = {} # contiendra longueur du mot vers la fréquence dans le dictionnaire\n",
    "for t,f in wordcounter.items():\n",
    "    lenfreq[len(t)] = lenfreq.get(len(t),0)+1 # si remplace +1 par +f, on a la relation longueur / fréquence dans les textes\n",
    "print(lenfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df = pd.DataFrame.from_dict(lenfreq, orient='index', columns=['frequency']).sort_values(by='frequency', ascending=False)\n",
    "freq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df.plot(kind='bar', title='notons la long tail!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Create training data (skip-gram style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we limit Y to be MEMORY_LIMIT x VOCAB_SIZE at most, otherwise we may lack memory\n",
    "# MEMORY_LIMIT = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROOM FOR IMPROVEMENT:\n",
    "\n",
    "**(maybe next year I'll do it, don't try it now, I'd say)**\n",
    "\n",
    "better than this generation of Y as a hard-coded numpy array (torch tensor), use this function at batch loading time: i.e., at every iteration of processing a batch, load some text files and convert them in that way.. \n",
    "\n",
    "Concretly, we should write a proper DataLoader (torch object) that does this. \n",
    "\n",
    "It's not a problem if all batches do not have exactly the same size\n",
    "\n",
    "## But today we do it dirty-style (hard-coded, all-in-memory-style)\n",
    "\n",
    "So, choose the variable `MEMORY_LIMIT` carefully, depending on how much RAM you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WINDOW_SIZE =3\n",
    "window_size=WINDOW_SIZE\n",
    "X = []\n",
    "Y = []\n",
    "filenumber = -1\n",
    "for f in textfiles:\n",
    "    filenumber+=1\n",
    "    with open(f, encoding='iso-8859-15') as f:\n",
    "        texte = f.read()\n",
    "        phrases = texte.split(\".\")\n",
    "        for phrase in phrases:\n",
    "            tokens = tokenize(phrase)\n",
    "            N = len(tokens)\n",
    "            if N >= 2*window_size+1:\n",
    "                ## replace unknown words with <unk>\n",
    "                for i, tok in enumerate(tokens):\n",
    "                    if tok not in vocabulary:\n",
    "                        tokens[i] = \"<unk>\"\n",
    "\n",
    "                ## fill in X and Y:\n",
    "                for i in range(window_size,N-window_size-1):\n",
    "                    X.append(tok2id[tokens[i]]) ## input\n",
    "                    ys = np.zeros(VOCAB_SIZE+1)\n",
    "                    for j in range(i-window_size, i+window_size+1):\n",
    "                        if i!=j: #  and 0<=j<N:\n",
    "                            ys[tok2id[tokens[j]]] += 1\n",
    "                    Y.append(ys)  ## context (to be predicted)\n",
    "    if len(Y) > MEMORY_LIMIT:\n",
    "        print(filenumber)\n",
    "        break\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(np.array(Y)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the difference between nn.Embedding vs nn.Linear: \n",
    "\n",
    "https://stackoverflow.com/questions/65445174/what-is-the-difference-between-an-embedding-layer-with-a-bias-immediately-afterw\n",
    "\n",
    "In this model, you should use:\n",
    "- a layer with `nn.Embedding`\n",
    "- a layer with nn.Linear (bias is useless)\n",
    "- no activation function is needed\n",
    "- even the softmax is implicit, it's hidden in the Loss (BCELoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class SkipGram_Model(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of Skip-Gram model described in paper:\n",
    "    https://arxiv.org/abs/1301.3781\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(SkipGram_Model, self).__init__()\n",
    "        ## TODO\n",
    "        \n",
    "        pass\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        ## TODO\n",
    "        return ??\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs, batch_size, criterion, optimizer, model, trainset):\n",
    "\n",
    "    X, Y = trainset\n",
    "    N = X.shape[0]\n",
    "    N -= X.shape[0]%batch_size\n",
    "    batch_number = X.shape[0]//batch_size    \n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    model.train() ## tells pytorch we are in train mode, not in test mode.\n",
    "    running_losses=[]\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "    \n",
    "        ## TODO: using batch size or just X,Y entirely, train the model\n",
    "        \n",
    "    t1 =time.time()\n",
    "    print('Finished Training. It took '+str(t1-t0)+\" seconds\")\n",
    "    return running_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = VOCAB_SIZE+1\n",
    "embed_dim = EMBED_DIM\n",
    "myOwnWord2Vec_model = SkipGram_Model(vocab_size=vocab_size, embed_dim=embed_dim)\n",
    "trainset = (X, Y)\n",
    "\n",
    "learning_rate=??\n",
    "num_epochs=3\n",
    "batch_size=??\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(myOwnWord2Vec_model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "running_losses = train(num_epochs, batch_size, criterion, optimizer, myOwnWord2Vec_model, trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(myOwnWord2Vec_model.state_dict, \"myOwnWord2Vec_model.torch.model_V=\"+str(vocab_size)+\"_d=\"+str(embed_dim))\n",
    "# vocab_size = VOCAB_SIZE+1\n",
    "# embed_dim = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying analogies... with small corpus, short training and small vocabulary...\n",
    "\n",
    "... It doesn't work very well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_vect(token):\n",
    "    vector = myOwnWord2Vec_model.embeddings(torch.tensor([tok2id[token]]))\n",
    "    vector_norm = np.linalg.norm(vector.detach())\n",
    "    return vector/vector_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = torch.tensor(np.arange((VOCAB_SIZE+1), dtype=int))\n",
    "all_vectors = np.array(myOwnWord2Vec_model.embeddings(all_words) .detach())\n",
    "normes = np.linalg.norm(all_vectors, axis=1).reshape(-1,1)\n",
    "all_vectors = all_vectors / normes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vecteur_cible = token_to_vect(\"roi\")+token_to_vect(\"femme\")-token_to_vect(\"homme\")\n",
    "vecteur_cible = np.array(vecteur_cible.detach())\n",
    "dot_prod = (all_vectors @ vecteur_cible.T)# / np.linalg.norm(vector.detach())\n",
    "inds = dot_prod[:,0].argsort()\n",
    "[id2tok[ind] for ind in np.array(inds)[::-1][:10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs: (for later, not today !)\n",
    "- make a DataLoader to use the whole corpus even on a small machine\n",
    "- learn with proper scheduler and for long enough ;)\n",
    "- save the model\n",
    "- load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To understand how things work: quick debug out of the loop !\n",
    "\n",
    "You can check for instance that your loss computes whay you want, here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = VOCAB_SIZE+1\n",
    "# embed_dim = 100\n",
    "# myOwnWord2Vec_model = SkipGram_Model(vocab_size=vocab_size, embed_dim=embed_dim) ## only 1 layer !!\n",
    "\n",
    "# learning_rate=3e-4\n",
    "# num_epochs=10\n",
    "# batch_size=128\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(myOwnWord2Vec_model.parameters(), lr=learning_rate)\n",
    "# trainset = (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## we try 1 epoch, just to check things are ok:\n",
    "# model = myOwnWord2Vec_model\n",
    "\n",
    "# optimizer.zero_grad()\n",
    "# y = model(X)\n",
    "\n",
    "# ## backward pass\n",
    "# loss = criterion(y, Y)  ## loss is also a torch object, so it also has the comput. graph.\n",
    "# loss.backward() # computes and stores the gradients next to the tensors of parameters themselves.\n",
    "# # at this point, the object 'model' has the gradients in it at the correct value.\n",
    "\n",
    "# ## update of gradients\n",
    "# optimizer.step() # updates the parameters wrt the local gradient\n",
    "# ## recall that we defined:  optimizer = optim.SGD(model.parameters(), ... so it knows about the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## we can compute the loss \"by hand\" to check it's what we expect \n",
    "# ## with all the pytorch things happening inside BCELoss, we can't be too sure...\n",
    "# softmax = nn.Softmax(dim=1)\n",
    "# softy = np.array(softmax(y.detach()))\n",
    "# softy.shape, softy.sum(1), softy.sum(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ## it does match :D\n",
    "# np.sum(np.array(Y)*np.log(softy))/X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# N  = X.shape[0]\n",
    "# batch_number = X.shape[0]//batch_size\n",
    "# X.shape[0]%batch_size\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
